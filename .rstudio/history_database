0:create(auth_token = "12faaec9d29ab5cce52a27ca51d1112fc83a2e93")
0:create <- function(pkg = ".", auth_token, private = FALSE){
0:devtools::use_github(pkg, auth_token = auth_token, protocol = "ssh", private = private)
0:rrtools::use_analysis()
0:rrtools::use_readme_rmd()
0:rrtools::use_dockerfile()
0:rrtools::use_travis()
0:devtools::use_testthat()
0:}
0:create(auth_token = "12faaec9d29ab5cce52a27ca51d1112fc83a2e93")
0:create(auth_token = "12faaec9d29ab5cce52a27ca51d1112fc83a2e93")
0:devtools::use_mit_license("Ben Marwick")
0:devtools::use_mit_license(copyright_holder="Ben Marwick")
0:library(saaabstracts)
0:# Chunk 1
0:knitr::opts_chunk$set(
0:collapse = TRUE,
0:warning = FALSE,
0:message = FALSE,
0:echo = FALSE,
0:comment = "#>",
0:fig.path = "figures"
0:)
0:library(saaabstracts)
0:library(tm)
0:library(topicmodels)
0:library(readxl)
0:library(Rmpfr)
0:library(ggplot2)
0:# Chunk 8
0:# Determine k number of topics
0:harmonicMean <- function(logLikelihoods, precision = 2000L) {
0:llMed <- median(logLikelihoods)
0:as.double(llMed - log(Rmpfr::mean(exp(-Rmpfr::mpfr(logLikelihoods,
0:prec = precision) + llMed))))
0:}
0:# Chunk 9
0:# run a loop over the abstract with different numbers of topics to find which number of topcis is the best
0:seqk <- seq(2, 100, 1)
0:burnin <- 1000
0:iter <- 1000
0:keep <- 50
0:# Chunk 11
0:fitted_many <- readRDS("../data/derived_data/fitted_many.rds")
0:# Chunk 12
0:# extract logliks from each topic
0:logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
0:# compute harmonic means
0:hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))
0:# to know the optimal number of topics:
0:optimal_number_of_topics <- seqk[which.max(hm_many)]
0:# plot it
0:library(ggplot2)
0:lda_plot <- ggplot(data.frame(seqk,
0:hm_many),
0:aes(x=seqk,
0:y=hm_many)) +
0:geom_path(lwd=1.5)  +
0:xlab('Number of Topics') +
0:ylab('Harmonic Mean') +
0:geom_vline(xintercept = seqk[which.max(hm_many)],
0:colour = "red") +
0:annotate("text",
0:x = 55,
0:y = -150000,
0:label = paste("The optimal number of topics is",
0:seqk[which.max(hm_many)])) +
0:theme_bw() +
0:ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of SAA General Abstracts", atop(italic("How many distinct topics in the abstracts?"), ""))))
0:getwd()
0:setwd("C:/emacs/saaabstracts/analysis/paper")
0:# Chunk 1
0:knitr::opts_chunk$set(
0:collapse = TRUE,
0:warning = FALSE,
0:message = FALSE,
0:echo = FALSE,
0:comment = "#>",
0:fig.path = "figures"
0:)
0:library(saaabstracts)
0:library(tm)
0:library(topicmodels)
0:library(readxl)
0:library(Rmpfr)
0:library(ggplot2)
0:# Chunk 8
0:# Determine k number of topics
0:harmonicMean <- function(logLikelihoods, precision = 2000L) {
0:llMed <- median(logLikelihoods)
0:as.double(llMed - log(Rmpfr::mean(exp(-Rmpfr::mpfr(logLikelihoods,
0:prec = precision) + llMed))))
0:}
0:# Chunk 9
0:# run a loop over the abstract with different numbers of topics to find which number of topcis is the best
0:seqk <- seq(2, 100, 1)
0:burnin <- 1000
0:iter <- 1000
0:keep <- 50
0:# Chunk 11
0:fitted_many <- readRDS("../data/derived_data/fitted_many.rds")
0:# Chunk 12
0:# extract logliks from each topic
0:logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
0:# compute harmonic means
0:hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))
0:# to know the optimal number of topics:
0:optimal_number_of_topics <- seqk[which.max(hm_many)]
0:# plot it
0:library(ggplot2)
0:lda_plot <- ggplot(data.frame(seqk,
0:hm_many),
0:aes(x=seqk,
0:y=hm_many)) +
0:geom_path(lwd=1.5)  +
0:xlab('Number of Topics') +
0:ylab('Harmonic Mean') +
0:geom_vline(xintercept = seqk[which.max(hm_many)],
0:colour = "red") +
0:annotate("text",
0:x = 55,
0:y = -150000,
0:label = paste("The optimal number of topics is",
0:seqk[which.max(hm_many)])) +
0:theme_bw() +
0:ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of SAA General Abstracts", atop(italic("How many distinct topics in the abstracts?"), ""))))
0:lda_plot
0:lda_plot
0:ggplot(data.frame(seqk,
0:hm_many),
0:aes(x=seqk,
0:y=hm_many)) +
0:geom_path(lwd=1.5)  +
0:xlab('Number of Topics') +
0:ylab('Harmonic Mean') +
0:geom_vline(xintercept = seqk[which.max(hm_many)],
0:colour = "red") +
0:annotate("text",
0:x = 55,
0:y = -150000,
0:label = paste("The optimal number of topics is",
0:seqk[which.max(hm_many)])) +
0:theme_bw()
0:tidy_documents
0:# Chunk 1
0:knitr::opts_chunk$set(
0:collapse = TRUE,
0:warning = FALSE,
0:message = FALSE,
0:echo = FALSE,
0:comment = "#>",
0:fig.path = "figures"
0:)
0:library(saaabstracts)
0:library(tm)
0:library(topicmodels)
0:library(readxl)
0:library(Rmpfr)
0:library(ggplot2)
0:# Chunk 7
0:general_reduced_dtm <- readRDS("../data/derived_data/general_reduced_dtm.rds")
0:# Chunk 8
0:# Determine k number of topics
0:harmonicMean <- function(logLikelihoods, precision = 2000L) {
0:llMed <- median(logLikelihoods)
0:as.double(llMed - log(Rmpfr::mean(exp(-Rmpfr::mpfr(logLikelihoods,
0:prec = precision) + llMed))))
0:}
0:# Chunk 9
0:# run a loop over the abstract with different numbers of topics to find which number of topcis is the best
0:seqk <- seq(2, 100, 1)
0:burnin <- 1000
0:iter <- 1000
0:keep <- 50
0:# Chunk 11
0:fitted_many <- readRDS("../data/derived_data/fitted_many.rds")
0:# Chunk 12
0:# extract logliks from each topic
0:logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
0:# compute harmonic means
0:hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))
0:# to know the optimal number of topics:
0:optimal_number_of_topics <- seqk[which.max(hm_many)]
0:# plot it
0:library(ggplot2)
0:lda_plot <- ggplot(data.frame(seqk,
0:hm_many),
0:aes(x=seqk,
0:y=hm_many)) +
0:geom_path(lwd=1.5)  +
0:xlab('Number of Topics') +
0:ylab('Harmonic Mean') +
0:geom_vline(xintercept = seqk[which.max(hm_many)],
0:colour = "red") +
0:annotate("text",
0:x = 55,
0:y = -150000,
0:label = paste("The optimal number of topics is",
0:seqk[which.max(hm_many)])) +
0:theme_bw() +
0:ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of SAA General Abstracts", atop(italic("How many distinct topics in the abstracts?"), ""))))
0:# Chunk 13
0:# now run the model with the optimum number of topics
0:system.time(general_model <- topicmodels::LDA(general_reduced_dtm,
0:optimal_number_of_topics,
0:method = "Gibbs",
0:control = list(iter=2000,
0:seed = 0622)))
0:# explore the model
0:general_topics <- topicmodels::topics(general_model, 1)
0:## In this case I am returning the top 30 terms.
0:general_terms <- as.data.frame(topicmodels::terms(general_model, 30),
0:stringsAsFactors = FALSE)
0:library(tidytext)
0:tidy_topics <- tidy(general_model, matrix = "beta")
0:library(ggplot2)
0:library(dplyr)
0:tidy_topics_top_terms <- tidy_topics %>%
0:group_by(topic) %>%
0:top_n(10, beta) %>%
0:ungroup() %>%
0:arrange(topic, -beta)
0:tidy_topics_top_terms
0:tidy_documents <- tidy(general_model, matrix = "gamma")
0:tidy_documents
0:tidy_documents %>%
0:spread(topic, gamma)
0:library(tidyr)
0:tidy_documents %>%
0:spread(topic, gamma)
0:tidy_documents_topics_gamma <-
0:tidy_documents %>%
0:spread(topic, gamma)
0:wss <- (nrow(tidy_documents_topics_gamma)-1)*sum(apply(tidy_documents_topics_gamma,2,var))
0:setwd("C:/emacs/saaabstracts/analysis/paper")
0:# Chunk 1
0:knitr::opts_chunk$set(
0:collapse = TRUE,
0:warning = FALSE,
0:message = FALSE,
0:echo = FALSE,
0:comment = "#>",
0:fig.path = "figures"
0:)
0:library(saaabstracts)
0:library(tm)
0:library(topicmodels)
0:library(readxl)
0:library(Rmpfr)
0:library(ggplot2)
0:# Chunk 7
0:general_reduced_dtm <- readRDS("../data/derived_data/general_reduced_dtm.rds")
0:# Chunk 8
0:# Determine k number of topics
0:harmonicMean <- function(logLikelihoods, precision = 2000L) {
0:llMed <- median(logLikelihoods)
0:as.double(llMed - log(Rmpfr::mean(exp(-Rmpfr::mpfr(logLikelihoods,
0:prec = precision) + llMed))))
0:}
0:# Chunk 9
0:# run a loop over the abstract with different numbers of topics to find which number of topcis is the best
0:seqk <- seq(2, 100, 1)
0:burnin <- 1000
0:iter <- 1000
0:keep <- 50
0:# Chunk 11
0:fitted_many <- readRDS("../data/derived_data/fitted_many.rds")
0:# Chunk 12
0:# extract logliks from each topic
0:logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
0:# compute harmonic means
0:hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))
0:# to know the optimal number of topics:
0:optimal_number_of_topics <- seqk[which.max(hm_many)]
0:# plot it
0:library(ggplot2)
0:lda_plot <- ggplot(data.frame(seqk,
0:hm_many),
0:aes(x=seqk,
0:y=hm_many)) +
0:geom_path(lwd=1.5)  +
0:xlab('Number of Topics') +
0:ylab('Harmonic Mean') +
0:geom_vline(xintercept = seqk[which.max(hm_many)],
0:colour = "red") +
0:annotate("text",
0:x = 55,
0:y = -150000,
0:label = paste("The optimal number of topics is",
0:seqk[which.max(hm_many)])) +
0:theme_bw() +
0:ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of SAA General Abstracts", atop(italic("How many distinct topics in the abstracts?"), ""))))
0:# Chunk 13
0:# now run the model with the optimum number of topics
0:system.time(general_model <- topicmodels::LDA(general_reduced_dtm,
0:optimal_number_of_topics,
0:method = "Gibbs",
0:control = list(iter=2000,
0:seed = 0622)))
0:# explore the model
0:general_topics <- topicmodels::topics(general_model, 1)
0:## In this case I am returning the top 30 terms.
0:general_terms <- as.data.frame(topicmodels::terms(general_model, 30),
0:stringsAsFactors = FALSE)
0:# Chunk 14
0:library(tidytext)
0:tidy_topics <- tidy(general_model, matrix = "beta")
0:library(ggplot2)
0:library(dplyr)
0:tidy_topics_top_terms <- tidy_topics %>%
0:group_by(topic) %>%
0:top_n(10, beta) %>%
0:ungroup() %>%
0:arrange(topic, -beta)
0:# Chunk 15
0:tidy_topics_top_terms %>%
0:mutate(term = reorder(term, beta)) %>%
0:ggplot(aes(term, beta, fill = factor(topic))) +
0:geom_col(show.legend = FALSE) +
0:facet_wrap(~ topic, scales = "free") +
0:coord_flip()
0:tidy_documents_topics_gamma
0:tidy_documents <- tidy(general_model, matrix = "gamma")
0:library(tidyr)
0:tidy_documents_topics_gamma <-
0:tidy_documents %>%
0:spread(topic, gamma)
0:abstract_classifications <-
0:tidy_documents %>%
0:group_by(document) %>%
0:top_n(1, gamma) %>%
0:ungroup() %>%
0:arrange(desc(gamma))
0:abstract_classifications
0:mydata <- tidy_documents_topics_gamma[,-1]
0:wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
0:for (i in 2:15) wss[i] <- sum(kmeans(mydata,
0:centers=i)$withinss)
0:plot(1:15, wss, type="b", xlab="Number of Clusters",
0:ylab="Within groups sum of squares")
0:install.packages("NbClust/")
0:install.packages("NbClust")
0:install.packages("NbClust")
0:install.packages("NbClust")
0:install.packages("NbClust")
0:install.packages("NbClust")
0:install.packages("NbClust")
0:library(NbClust)
0:NbClust(mydata)
0:NbClust(mydata, method = "kmeans")
0:mydata <- tidy_documents_topics_gamma[,-1]
0:# Chunk 1
0:knitr::opts_chunk$set(
0:collapse = TRUE,
0:warning = FALSE,
0:message = FALSE,
0:echo = FALSE,
0:comment = "#>",
0:fig.path = "figures"
0:)
0:library(saaabstracts)
0:library(tm)
0:library(topicmodels)
0:library(readxl)
0:library(Rmpfr)
0:library(ggplot2)
0:# Chunk 7
0:general_reduced_dtm <- readRDS("../data/derived_data/general_reduced_dtm.rds")
0:# Chunk 8
0:# Determine k number of topics
0:harmonicMean <- function(logLikelihoods, precision = 2000L) {
0:llMed <- median(logLikelihoods)
0:as.double(llMed - log(Rmpfr::mean(exp(-Rmpfr::mpfr(logLikelihoods,
0:prec = precision) + llMed))))
0:}
0:# Chunk 9
0:# run a loop over the abstract with different numbers of topics to find which number of topcis is the best
0:seqk <- seq(2, 100, 1)
0:burnin <- 1000
0:iter <- 1000
0:keep <- 50
0:# Chunk 11
0:fitted_many <- readRDS("../data/derived_data/fitted_many.rds")
0:# Chunk 12
0:# extract logliks from each topic
0:logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
0:# compute harmonic means
0:hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))
0:# to know the optimal number of topics:
0:optimal_number_of_topics <- seqk[which.max(hm_many)]
0:# plot it
0:library(ggplot2)
0:lda_plot <- ggplot(data.frame(seqk,
0:hm_many),
0:aes(x=seqk,
0:y=hm_many)) +
0:geom_path(lwd=1.5)  +
0:xlab('Number of Topics') +
0:ylab('Harmonic Mean') +
0:geom_vline(xintercept = seqk[which.max(hm_many)],
0:colour = "red") +
0:annotate("text",
0:x = 55,
0:y = -150000,
0:label = paste("The optimal number of topics is",
0:seqk[which.max(hm_many)])) +
0:theme_bw() +
0:ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of SAA General Abstracts", atop(italic("How many distinct topics in the abstracts?"), ""))))
0:# Chunk 13
0:# now run the model with the optimum number of topics
0:system.time(general_model <- topicmodels::LDA(general_reduced_dtm,
0:optimal_number_of_topics,
0:method = "Gibbs",
0:control = list(iter=2000,
0:seed = 0622)))
0:# explore the model
0:general_topics <- topicmodels::topics(general_model, 1)
0:## In this case I am returning the top 30 terms.
0:general_terms <- as.data.frame(topicmodels::terms(general_model, 30),
0:stringsAsFactors = FALSE)
0:# Chunk 14
0:library(tidytext)
0:tidy_topics <- tidy(general_model, matrix = "beta")
0:library(ggplot2)
0:library(dplyr)
0:tidy_topics_top_terms <- tidy_topics %>%
0:group_by(topic) %>%
0:top_n(10, beta) %>%
0:ungroup() %>%
0:arrange(topic, -beta)
0:# Chunk 15
0:tidy_topics_top_terms %>%
0:mutate(term = reorder(term, beta)) %>%
0:ggplot(aes(term, beta, fill = factor(topic))) +
0:geom_col(show.legend = FALSE) +
0:facet_wrap(~ topic, scales = "free") +
0:coord_flip()
0:# Chunk 16
0:tidy_documents <- tidy(general_model, matrix = "gamma")
0:library(tidyr)
0:tidy_documents_topics_gamma <-
0:tidy_documents %>%
0:spread(topic, gamma)
0:abstract_classifications <-
0:tidy_documents %>%
0:group_by(document) %>%
0:top_n(1, gamma) %>%
0:ungroup() %>%
0:arrange(desc(gamma))
0:mydata <- tidy_documents_topics_gamma[,-1]
0:library(NbClust)
0:NbClust(mydata, method = "kmeans")
0:nclust <- NbClust(mydata[ , 1:5], method = "kmeans")
0:nclust <- NbClust(mydata[ , 1:3], method = "kmeans")
0:nclust <- NbClust(mydata[ , 1:3], method = "Ward")
0:nclust <- NbClust(mydata[ , 1:3], method = "ward.D")
0:nclust <- NbClust(mydata[ , 1:3], method = "ward.D", index =  "gap")
0:nclust
0:nclust <- NbClust(mydata[ , 1:5], method = "ward.D", index =  "gap")
0:nclust
0:library(fpc)
0:pamk.best <- pamk(mydata)
0:pamk.best$nc
0:mydata
0:plot(pam(mydata[,1:2], pamk.best$nc))
0:nclust <- NbClust(mydata, method = "ward.D", index =  "gap")
0:nclust
0:nclust <- NbClust(mydata, method = "kmeans")
0:mtcars %>%
0:filter(mpg > 50)
0:mtcars %>%
0:filter(mpg > 10)
0:mtcars %>%
0:filter(mpg > 10) %>%
0:select(cyl)
1501423832625:fitted_many <- readRDS("../data/derived_data/fitted_many.rds")
