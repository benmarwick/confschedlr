---
title: "Title Goes Here"
author:
  - author 1
  - author 2
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::html_document2:
    fig_caption: yes
    reference_docx: templates/template.docx
bibliography: references.bib
csl: journal-of-archaeological-science.csl
abstract: |
  Text of abstract
keywords: |
  keyword 1; keyword 2; keyword 3
highlights: |
  These are the highlights. 
---


<!-- This is the format for text comments that will be ignored during renderings. Do not put R code in these comments because it will not be ignored. -->

```{r, setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "figures"
)

library(saaabstracts)
library(tm)
library(topicmodels)
library(readxl)
library(Rmpfr)
library(ggplot2)
```

# Introduction

```{r eval=FALSE}
# read in the abstracts
general <- read_excel("../data/raw_data/General Abstracts.xlsx")
```

```{r eval=FALSE}
# prepare a mapping structure, first item is column with text to model, others are metadata
m <- list(content = "Abstract",  # content
          id = "Abstract Id",      # metadata
          title = "Title", 
          author_first = "First Name",
          author_last = "Last Name",
          geo = "Geographic Focus",
          keyword1 = "Keyword1", 
          keyword2 = "Keyword2", 
          keyword3 = "Keyword3", 
          org = "Affiliation")
```

```{r eval=FALSE}
# Create a corpus object from the abstracts
general_corpus <- tm::Corpus(tm::DataframeSource(general), 
                             readerControl = list(reader = tm::readTabular(mapping = m)))

# create a document term matrix
general_tm <- tm::DocumentTermMatrix(general_corpus, 
                                     control = list(stemming = TRUE, 
                                                    stopwords = TRUE,
                                                    minWordLength = 2, 
                      s                              removeNumbers = TRUE, 
                                                    removePunctuation = TRUE))
```



```{r eval=FALSE}
# Use TF-IDF to wieght terms and remove rare ones
term_tfidf <- 
  tapply(general_tm$v/slam::row_sums(general_tm)[general_tm$i], 
         general_tm$j, mean) * log2(tm::nDocs(general_tm)/slam::col_sums(general_tm > 0))

summary(term_tfidf)
# Median =  0.08989
```


```{r eval=FALSE }
## Keeping the rows with tfidf >= median
general_reduced_dtm <- general_tm[,term_tfidf >= summary(term_tfidf)[3]]
summary(slam::col_sums(general_reduced_dtm))
# save
saveRDS(general_reduced_dtm, "../data/derived_data/general_reduced_dtm.rds")
```

```{r}
general_reduced_dtm <- readRDS("../data/derived_data/general_reduced_dtm.rds")
```


```{r}
library(ldatuning)
result <- FindTopicsNumber(
  general_reduced_dtm,
  topics = seq(from = 2, to = 100, by = 2),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

knitr::kable(result)
FindTopicsNumber_plot(result)
```



```{r}
# Determine k number of topics
harmonicMean <- function(logLikelihoods, precision = 2000L) {
  llMed <- median(logLikelihoods)
  as.double(llMed - log(Rmpfr::mean(exp(-Rmpfr::mpfr(logLikelihoods,
                                       prec = precision) + llMed))))
}
```




```{r}
# run a loop over the abstract with different numbers of topics to find which number of topcis is the best
seqk <- seq(2, 100, 1)
burnin <- 1000
iter <- 1000
keep <- 50
```

```{r eval=FALSE }
system.time(fitted_many <- 
              lapply(seqk, function(k) topicmodels::LDA(general_reduced_dtm, 
                                                        k = k,
                                                        method = "Gibbs",
                                                        control = list(burnin = burnin,
                                                                       iter = iter, 
                                                                       keep = keep) )))
                                                                       
saveRDS(fitted_many, "../data/derived_data/fitted_many.rds")
```

```{r}
fitted_many <- readRDS("../data/derived_data/fitted_many.rds")
```

```{r}
# extract logliks from each topic
logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])

# compute harmonic means
hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))

# to know the optimal number of topics:
optimal_number_of_topics <- seqk[which.max(hm_many)]

# plot it
library(ggplot2)
lda_plot <- ggplot(data.frame(seqk, 
                             hm_many), 
                  aes(x=seqk, 
                      y=hm_many)) + 
  geom_path(lwd=1.5)  +
  xlab('Number of Topics') +
  ylab('Harmonic Mean') +
  geom_vline(xintercept = seqk[which.max(hm_many)],
             colour = "red") +
  annotate("text",
           x = 55, 
           y = -150000, 
           label = paste("The optimal number of topics is", 
                         seqk[which.max(hm_many)])) +
  theme_bw() +
  ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of SAA General Abstracts", atop(italic("How many distinct topics in the abstracts?"), ""))))
```

```{r}
# now run the model with the optimum number of topics
system.time(general_model <- topicmodels::LDA(general_reduced_dtm, 
                                              optimal_number_of_topics,
                                              method = "Gibbs", 
                                              control = list(iter=2000, 
                                                             seed = 0622)))

# explore the model
general_topics <- topicmodels::topics(general_model, 1)

## In this case I am returning the top 30 terms.
general_terms <- as.data.frame(topicmodels::terms(general_model, 30), 
                               stringsAsFactors = FALSE)
```


```{r}
library(tidytext)
tidy_topics <- tidy(general_model, matrix = "beta")

library(ggplot2)
library(dplyr)

tidy_topics_top_terms <- tidy_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```


```{r}
tidy_topics_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

```{r}
tidy_documents <- tidy(general_model, matrix = "gamma")

library(tidyr)
tidy_documents_topics_gamma <-
  tidy_documents %>% 
    spread(topic, gamma)
    
abstract_classifications <- 
  tidy_documents %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  ungroup() %>% 
  arrange(desc(gamma))
```

```{r eval=FALSE}
mydata <- tidy_documents_topics_gamma[,-1]

# Determine number of clusters
library(NbClust)
nclust <- NbClust(mydata, method = "kmeans")

library(fpc)
pamk.best <- pamk(mydata)

# Determine number of clusters
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata, 
  	centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")

# K-Means Cluster Analysis
fit <- kmeans(mydata, 5) # 5 cluster solution
# get cluster means 
aggregate(mydata,by=list(fit$cluster),FUN=mean)
# append cluster assignment
mydata <- data.frame(mydata, fit$cluster)
```


## Wokring in a local Docker container

To run this project in a local Docker container, I start a bash shell in the project directory:

Do this one time only, build my Docker container from Dockerfile at top-level in my project:

```
docker build - < Dockerfile
docker ps
```

Take note of the container ID, which you get after `docker ps`

Then run my Docker container, access it via the web browser, link the project dir to the Docker drive. Make sure you're in a local directory that is sharable (C:\Users\yourname is usually good):

```
docker run -dp 8787:8787 -v ${pwd}:/home/rstudio/ -e ROOT=TRUE  <container ID>
```

Then go to `http://192.168.99.100:8787/` or `localhost:8787` in youor browser, log in (rstudio/rstudio), and start your RStudio project by double-clickin on the .Rproj file

When you're done, in the shell, stop all docker containers with:

```
docker stop $(docker ps -a -q)
```

Or do `docker ps` and `docker stop <container ID>` just to stop one container. 

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, cache = FALSE}
# which R packages and versions?
devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? You may need to change the path value
# if your Rmd is not in analysis/paper/
git2r::repository("../..")
```

